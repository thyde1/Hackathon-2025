{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step to ensure that the venv is being used for the project not local copies, should point at .venv in project.\n",
    "import sys, shutil\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"uv:\", shutil.which(\"uv\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs into the current Jupyter kernel environment\n",
    "%pip install -U uv \n",
    "#! to run shell commands\n",
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fdf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain + MCP Setup for Attractions Booking (HTTP-based for Jupyter)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Official MCP adapter imports for HTTP transport\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Updated imports with official MCP adapter loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Client Setup using Official Adapter with HTTP Transport\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Global MCP client for HTTP\n",
    "mcp_client = None\n",
    "\n",
    "async def create_mcp_tools():\n",
    "    \"\"\"Create MCP tools using the official LangChain MCP adapter with HTTP transport\"\"\"\n",
    "    global mcp_client\n",
    "    \n",
    "    try:\n",
    "        # Create MultiServerMCPClient with streamable_http transport\n",
    "        mcp_client = MultiServerMCPClient({\n",
    "            \"attractions\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": os.getenv(\"ATTRACTIONS_MCP_URL\")\n",
    "            },\n",
    "            \"weather\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": os.getenv(\"WEATHER_MCP_URL\")\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Get tools from the MCP server\n",
    "        tools = await mcp_client.get_tools()\n",
    "        print(f\"Loaded {len(tools)} MCP tools: {[tool.name for tool in tools]}\")\n",
    "        return tools\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to MCP HTTP server: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"ðŸ”— MCP HTTP adapter setup ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deceb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MCP Tools using Official Adapter\n",
    "# The tools will be loaded dynamically when setting up the agent\n",
    "# No need to manually create tool wrappers - the adapter handles this automatically\n",
    "print(\"ðŸ› ï¸ Ready to load MCP tools via official adapter!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1bb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_agent():\n",
    "    \"\"\"Setup LangChain agent with MCP tools using official adapter\"\"\"\n",
    "    \n",
    "    # Initialize LLM for Azure OpenAI\n",
    "    # can get this from Azure Open Ai service -> Azure Ai Foundary Portal\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    \n",
    "    llm = AzureChatOpenAI(\n",
    "        deployment_name=os.getenv(\"DEPLOYMENT_NAME\"),  # Your Azure deployment name\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "        api_version=os.getenv(\"AZURE_API_VERSION\"), \n",
    "        temperature=1\n",
    "    )\n",
    "    \n",
    "    # Load MCP tools using official adapter\n",
    "    tools = await create_mcp_tools()\n",
    "    \n",
    "    if not tools:\n",
    "        print(\"No MCP tools loaded. Make sure the MCP server is accessible.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loaded {len(tools)} MCP tools: {[tool.name for tool in tools]}\")\n",
    "    \n",
    "    # Create system prompt\n",
    "    system_prompt = \"\"\"You are a helpful travel assistant that can help users find and book attractions including weather.\n",
    "    \n",
    "    You have access to multiple MCP tools for tourist attractions, including:\n",
    "    - Searching for attractions by location and category\n",
    "    - Getting detailed attraction information\n",
    "    - Booking attractions for visitors\n",
    "    - Getting random attraction suggestions\n",
    "    - And more...\n",
    "    \n",
    "    When users ask about travel plans, use these tools to provide comprehensive information.\n",
    "    Always be helpful and provide practical advice.\"\"\"\n",
    "    \n",
    "    # Create prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "    \n",
    "    # Create agent\n",
    "    agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "    # memory\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    # Create agent executor with tool logging callback and verbose output\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "    \n",
    "    return agent_executor\n",
    "\n",
    "# Initialize the agent (now async)\n",
    "agent_executor = None\n",
    "print(\"ðŸ¤– Agent setup function ready! Run the next cell to initialize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent with MCP tools\n",
    "async def initialize_agent():\n",
    "    \"\"\"Initialize the agent with MCP tools\"\"\"\n",
    "    global agent_executor\n",
    "    print(\"Initializing agent with MCP tools...\")\n",
    "    agent_executor = await setup_agent()\n",
    "    if agent_executor:\n",
    "        print(\"LangChain agent with MCP tools ready!\")\n",
    "    else:\n",
    "        print(\"Failed to initialize agent. Check MCP server connection.\")\n",
    "\n",
    "# Run the initialization\n",
    "await initialize_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Input Handler\n",
    "async def process_user_input(user_input: str) -> str:\n",
    "    \"\"\"Process user input and return LLM response using MCP tools\"\"\"\n",
    "    if not agent_executor:\n",
    "        return \"Agent not initialized. Please run the initialization cell first.\"\n",
    "    \n",
    "    try:\n",
    "        # Use the agent to process the input and get intermediate steps\n",
    "        result = await agent_executor.ainvoke({\"input\": user_input})\n",
    "        output = result.get(\"output\") or result.get(\"final_output\") or \"\"\n",
    "\n",
    "        # Print intermediate steps if present\n",
    "        steps = result.get(\"intermediate_steps\") or []\n",
    "        for step in steps:\n",
    "            action = None\n",
    "            observation = None\n",
    "            if isinstance(step, tuple) and len(step) == 2:\n",
    "                action, observation = step\n",
    "            elif isinstance(step, dict) and \"action\" in step:\n",
    "                action = step.get(\"action\")\n",
    "                observation = step.get(\"observation\")\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            tool_name = getattr(action, \"tool\", getattr(action, \"tool_name\", \"unknown\"))\n",
    "            tool_args = getattr(action, \"tool_input\", getattr(action, \"input\", None))\n",
    "            print(f\"\\n--- Tool: {tool_name}\")\n",
    "            print(f\"args: {tool_args}\")\n",
    "            if observation is not None:\n",
    "                print(f\"result: {observation}\")\n",
    "            print(\"---\\n\")\n",
    "\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        return f\"Error processing request: {str(e)}\"\n",
    "\n",
    "# Interactive function for easy testing\n",
    "async def ask_assistant(question: str):\n",
    "    \"\"\"Easy-to-use function for asking the travel assistant\"\"\"\n",
    "    print(f\"ðŸ§³ User: {question}\")\n",
    "    print(\"ðŸ¤– Assistant:\")\n",
    "    \n",
    "    response = await process_user_input(question)\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "print(\"ðŸ’¬ User input handler ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4447a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MCP server connectivity and tools\n",
    "async def test_mcp_connection():\n",
    "    \"\"\"Test MCP server connection and list available tools\"\"\"\n",
    "    tools = await create_mcp_tools()\n",
    "    if tools:\n",
    "        print(f\"MCP HTTP server connected successfully!\")\n",
    "        print(f\"Available tools: {[tool.name for tool in tools]}\")\n",
    "        for tool in tools:\n",
    "            print(f\"  - {tool.name}: {tool.description}\")\n",
    "    else:\n",
    "        print(\"Failed to connect to MCP HTTP server\")\n",
    "\n",
    "# Test MCP HTTP connection\n",
    "await test_mcp_connection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dcf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ EXAMPLE USAGE - Run this cell after setting up your API key!\n",
    "\n",
    "# Simple question\n",
    "await ask_assistant(\"What are some popular attractions in Paris? and what is the weather?\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Weather + attractions\n",
    "# await ask_assistant(\"I'm planning to visit paris tomorrow. What's the weather like and what attractions should I visit?\")\n",
    "# print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0841c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat loop â€” keep asking questions until you exit\n",
    "\n",
    "# try\n",
    "# give me a random attraction\n",
    "# what about attractions in paris\n",
    "# more details on eiffel tower\n",
    "# book eiffel tower\n",
    "\n",
    "\n",
    "async def chat_loop():\n",
    "    print(\"Type 'exit' to quit. Press Enter on an empty line to skip.\")\n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"You: \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            print(\"\\nExiting.\")\n",
    "            break\n",
    "        if not question:\n",
    "            continue\n",
    "        if question.lower() in (\"exit\", \"quit\", \"q\"):\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        await ask_assistant(question)\n",
    "\n",
    "# Start the loop\n",
    "await chat_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968731b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup function for HTTP MCP client\n",
    "async def cleanup_mcp():\n",
    "    \"\"\"Cleanup MCP client and server resources\"\"\"\n",
    "    global mcp_client\n",
    "    if mcp_client:\n",
    "        try:\n",
    "            await mcp_client.close()\n",
    "            print(\"MCP client closed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error closing MCP client: {e}\")\n",
    "        mcp_client = None\n",
    "\n",
    "print(\"ðŸ§¹ Cleanup function ready!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
